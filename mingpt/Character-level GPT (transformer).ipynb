{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset): # build custom dataset\n",
    "    \n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = {ch : i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i : ch for i,ch in enumerate(chars)}\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the modle for itx context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# download text file for training model\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "DATA_PATH = Path(\"../data\")\n",
    "PATH = DATA_PATH / \"gpt\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/\"\n",
    "FILENAME = \"input.txt\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    urlretrieve(URL + FILENAME, PATH / FILENAME)\n",
    "\n",
    "with open(PATH / FILENAME, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/26/2021 17:17:32 - INFO - model - number of parameters: 2.535219e+07\n"
     ]
    }
   ],
   "source": [
    "from model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, block_size,\n",
    "                 n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = \"model/params.ckpt\"\n",
    "model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modified TrainerConfig arguments for reducing memory\n",
    "from trainer import Trainer, TrainerConfig\n",
    "tconf = TrainerConfig(max_epochs=0, batch_size=256, learning_rate=6e-4,\n",
    "                     lr_decay=True, warmup_tokens=512*20,\n",
    "                     final_tokens=2*len(train_dataset)*block_size,\n",
    "                     ckpt_path=\"model/params.ckpt\",\n",
    "                     num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O god, O God! that e'er this tongue of mine,\n",
      "That laid the sentence of dread banishment\n",
      "On yon proud man, should take it off again\n",
      "With words of sooth! O that I were as great\n",
      "As is my grief, or lesser than my name!\n",
      "Or that I could forget what I have been,\n",
      "Or not remember what I must be now!\n",
      "Swell'st thou, proud heart? I'll give thee scope to beat,\n",
      "Since fear it not be in being beheld\n",
      "To say 'Beseech you, cease.' You have made fair hands,\n",
      "You and your crafts! you have crafted fair!\n",
      "\n",
      "COMINIUS:\n",
      "You have brought\n",
      "A trembling upon Rome, such as was never\n",
      "So incapable of help.\n",
      "\n",
      "Both Tribunes:\n",
      "Say not we brought it.\n",
      "\n",
      "MENENIUS:\n",
      "How! Was it we? we loved him but, like beasts\n",
      "And cowardly nobles, gave way unto your clusters,\n",
      "Who did hoot him out o' the city.\n",
      "\n",
      "COMINIUS:\n",
      "But I fear\n",
      "They'll roar him in again. Tullus Aufidius,\n",
      "The second name of men, obeys his points\n",
      "As if he were his officer: desperation\n",
      "Is all the policy, strength and defence,\n",
      "That Rome can make against them.\n",
      "\n",
      "MENENIUS:\n",
      "Here come the clusters.\n",
      "And is Aufidius with him? You are they\n",
      "That made the air unwholesome, when you cast\n",
      "Your stinking greasy caps in hooting at\n",
      "Coriolanus' exile. Now he's coming;\n",
      "And not a hair upon a soldier's head\n",
      "Which will not prove a whip: as many coxcombs\n",
      "As you threw caps up will he tumble down,\n",
      "And pay you for your voices. 'Tis no matter;\n",
      "if he could burn us all into one coal,\n",
      "We have deserved it.\n",
      "\n",
      "Citizens:\n",
      "Faith, we hear fearful news.\n",
      "\n",
      "First Citizen:\n",
      "For mine own part,\n",
      "When I said, banish him, I said 'twas pity.\n",
      "\n",
      "Second Citizen:\n",
      "And so did I.\n",
      "\n",
      "Third Citizen:\n",
      "And so did I; and, to say the truth, so did very\n",
      "many of us: that we did, we did for the best; and\n",
      "though we willingly consented to his banishment, yet\n",
      "it was against our will.\n",
      "\n",
      "COMINIUS:\n",
      "Ye re goodly things, you voices!\n",
      "\n",
      "MENENIUS:\n",
      "You have made\n",
      "Good work, you and your cry! Shall's to the Capitol?\n",
      "\n",
      "COMINIUS:\n",
      "O, ay, what else?\n",
      "\n",
      "SICINIUS:\n",
      "Go, masters, get you home; be not dismay'd:\n",
      "These are a side that would be glad to have\n",
      "This true which th\n"
     ]
    }
   ],
   "source": [
    "from utils import sample\n",
    "\n",
    "context = \"O god, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
